当前业界在长文本故事生成领域面临多重核心痛点：一方面，生成高质量、数千词级的连贯故事需要兼顾情节跟踪、人物弧线发展与风格一致性等多种能力，但现有大语言模型（LLMs）生成的内容常存在原创性不足、情节逻辑断裂、人物塑造单薄、节奏失衡等问题，还伴随重复、质量退化等长文本生成的共性缺陷；另一方面，强化学习（RL）在该领域难以落地，原因在于故事 “正确性” 缺乏明确界定，标注高质量故事续篇数据集成本极高，且叙事的多维度、精细化特性导致难以设计单一有效的奖励函数，现有方法多依赖手工设计的提示技术模拟人类写作流程（如起草、编辑、规划），灵活性和泛化性不足，且未针对故事生成的推理过程进行专门优化。本工作的核心创新点在于针对性提出了 “任务设计 + 奖励机制” 的双重解决方案：一是提出 “下一章预测（Next-Chapter Prediction, NCP）” 任务，将复杂的长文本故事生成拆解为更易处理的子任务，借鉴人类写作中 “全局框架 + 局部细节” 的思路，整合全局故事梗概、前文摘要、人物档案、上一章文本和下一章概要等多维度故事信息，聚焦于基于这些信息预测下一章内容，大幅降低了长文本生成的建模难度；二是设计 “基于完成可能性提升的可验证奖励（Verifiable Rewards via Completion Likelihood Improvement, VR-CLI）”，无需依赖标注数据或明确的外部奖励，通过计算生成模型在 “故事信息 + 推理计划” 条件下对真实下一章的困惑度提升比例作为奖励，巧妙规避了故事质量难以量化的问题；三是采用 GRPO 算法训练推理模型，让模型先对故事信息进行推理并生成详细的下一章计划，再将该计划作为上下文输入基础生成模型，实现了推理过程与生成过程的分离优化。这套方案通过 NCP 任务简化了长文本生成的复杂度，通过 VR-CLI 解决了奖励函数设计的核心难题，再借助推理计划的引导提升了生成内容的逻辑性和连贯性。实验结果显示，经该方法训练的模型（尤其 7B 规模）在情节、创造力、人物塑造等多个维度的人类偏好评分中显著优于基线模型（包括无推理基线、未训练推理模型和监督微调模型），且在科幻、奇幻等需要强逻辑和世界观构建的题材中效果尤为突出，有效缓解了现有长文本故事生成的核心痛点。

Paper: [Learning to Reason for Long-Form Story Generation](https://doi.org/10.48550/arXiv.2503.22828)

## 长文本故事生成领域业界痛点

### 长文本故事生成核心能力先天不足

长文本故事生成需同时满足多维度创作要求，但现有 LLM 的生成结果普遍存在质量短板，且难以突破长文本固有的技术瓶颈。

- **情节与角色发展失衡**: 高质量故事需兼顾逻辑连贯的情节推进、立体的角色弧光及沉浸式世界观构建，但 LLM 生成内容常出现情节断裂、角色行为前后矛盾、主题表达浅薄等问题，难以平衡 “故事性” 与 “创造性”。

- **原创性与风格一致性缺失**: 生成内容易陷入俗套叙事模式，原创性不足；同时难以维持跨章节的风格统一，存在语言表达波动、叙事节奏混乱等问题。

- **长文本生成基础缺陷**: 随着生成长度增加，普遍出现文本重复、信息冗余、质量梯度下降等问题，无法稳定输出数千词级别的连贯叙事。

### 强化学习方法的应用存在难以逾越的障碍

RL 在 LLM 优化中表现突出，但长文本故事生成的特性使其难以直接适配，核心障碍集中在奖励机制与数据获取两方面。

- **难以设计奖励**: 故事质量具有强主观性和多维度特性，“正确性” 无法量化定义 —— 既无明确的 “正确答案” 作为判断标准，也难以用单一指标涵盖情节、创意、语言等多维度要求，导致传统 RL 的 “可验证奖励函数” 无法迁移。

- **数据标注成本极高**: 故事续写的可能性具有开放性和多样性，收集大规模带偏好标签、奖励值或推理轨迹的标注数据集，不仅耗时耗力，还面临标注标准难以统一的问题，实用性极低。

- **RL 任务适配性差**: 现有 RL 方法要么依赖高质量标注数据集（如 RLHF），要么依赖明确的正确性验证（如 RLVR 在数学 / 编码领域的应用），均无法适配长文本故事生成 “无标准答案、需创意发挥” 的核心特性。

### 现有方法依赖人工设计, 泛化性弱

当前主流方案未脱离 “人工设计主导” 的框架，缺乏自动化、通用化的有效路径。

- **过度依赖提示工程**: 多数方法通过人工设计提示词，模拟人类写作的起草、编辑、规划等环节，需针对具体任务场景定制化设计，过程繁琐且高度任务依赖，缺少泛化能力。

- **写作流程拆分效率低下**: 部分工作尝试将写作拆分为多子任务，通过多智能体协作优化，但子任务划分、协作逻辑仍需人工定义，无法自适应不同故事类型、体裁的创作需求。

- **模型优化易陷入过拟合**: 直接对 “故事生成” 任务进行监督微调（SFT），易导致模型学习到数据集的特定风格或情节模式，出现过拟合，无法泛化到新的故事场景。

### 评估体系不完善, 无法支撑技术迭代验证

故事质量的主观性导致评估环节成为技术推进的 “瓶颈”，缺乏高效、客观、可复现的评估方案。

- **评估方式耗时且主观**: 目前业界普遍依赖人工 pairwise 比较进行评估，需标注者从情节、创意、语言等多维度判断偏好，不仅耗时耗力，还受标注者个人审美、认知影响，结果一致性低。

- **缺乏有效的自动评估指标**: 传统文本生成指标（如 BLEU、Rouge）无法捕捉故事的 “连贯性”“创造性”“角色一致性” 等核心维度，与人类判断相关性极低，难以作为模型优化的有效反馈信号。

- **评估范围局限**: 现有评估多聚焦短文本或单章节生成质量，缺乏对跨章节叙事连贯性、长期情节布局合理性的评估方法，无法全面反映长文本故事生成能力。

## 领域进展和相关工作

### 故事生成领域研究进展

故事生成的研究核心是从 “短文本补全” 向 “长文本连贯叙事” 演进，关键挑战包括情节连贯性、角色一致性、世界观构建及评估主观性.

1. **早期基础: 以文本补全任务实现故事生成**

    Mostafazadeh et al. 构建了首个用于常识故事理解的数据集 ROCStories $^{[2]}$，包含 5 句短故事及补全任务，首次定义了 “故事连贯性” 的基础评估标准（如情节逻辑、角色行为一致性）。但其仅支持短文本, 无法扩展到长文本, 且未涉及角色弧光和世界观构建等.

2. **长文本生成突破：融入浓缩故事信息**

    随着长上下文 LLM（如 GPT-4、Llama 3）的发展，研究开始通过 “融入高维故事信息” 提升长文本质量，核心是用 “情节大纲、角色档案” 等结构化信息引导生成。

    层级神经故事生成（Hierarchical Neural Story Generation）$^{[3]}$首次将 “情节大纲（Plot Outlines）” 作为输入，先生成大纲再扩展为故事文本，解决了长文本情节碎片化问题。




## 参考文献

[[1](https://doi.org/10.18653/v1/2020.emnlp-main.175)] Alexander Gurung, Mirella Lapata. Learning to Reason for Long-Form Story Generation. *arXiv*, 2025.

[[2](https://doi.org/10.18653/v1/N16-1098)] Mostafazadeh, N., Chambers, N., He, X., et al. A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.*, 2016.

[[3](https://doi.org/10.18653/v1/P18-1082)] Fan, A., Lewis, M., & Dauphin, Y. Hierarchical Neural Story Generation. *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 2018.

