---
layout: post
toc: true
title: "深入线性回归模型, 最小二乘法和梯度下降法求解"
categories: Math
tags: [Math, MachineLearning, DeepLearning]
author:
  - vortezwohl
  - 吴子豪
---
线性回归模型是数学/统计学/机器学习中的一类十分基础且重要的模型, 利用线性回归模型, 可以从一组输入变量 $x$ 的线性组合中计算输出变量 $y$.

## 模型定义

线性回归 (Linear Regression) 是一种线性模型, 它假设输入变量 $x$ 和单个输出变量 $y$ 之间存在线性关系. 具体来说, 利用线性回归模型, 可以从一组输入变量 $x$ 的线性组合中, 计算输出变量 $y$.

$$
y = wx + b\\
f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b
$$

## 线性方程求解

假设我们有如下二元一次方程:

$$
y = wx + b
$$

已知有两组数据:

$$
x = 1 时, y = 3\\
x = 2 时, y = 5
$$

将数据代入方程得到:

$$
w + b = 3\\
2w + b =5
$$

解得:

$$
w = 2, b = 1
$$

现在当我们有任意一个 $x$, 均可以计算出其对应的 $y$, 例如 $x=5$ 时, $y=11$.

## 线性回归模型

给定 $d$ 个特征描述的样本 $x = (x1,x2,...,x_d)$, 其中 $x_i$ 是 $x$ 在第 $i$ 个特征上的取值, 线性模型试图通过学得一个通过特征的线性组合来进行预测的函数, 即:

$$
f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b
$$

一般用矩阵形式写成:

$$
f(x) = w^Tx + b\\
w = (w_1, w_2, ..., w_d)
$$

其中特征和结果均线性(不大于一次幂). $w$ 和 $b$ 都学得后, 模型就得以确定. **许多更为强大的非线性模型都是在线性模型的基础上引入层级结构或高维映射得到**.

## 最小二乘法

基于均方误差最小化来进行模型求解的方法称为**最小二乘法(Least Square Method)**, 它的主要思想就是将**模型求解**抽象为一个**优化问题**, 其选择未知参数, 使理论值和观测值之差的平方和达到最小.

假设输入特征的维度为 1:

$$
f(x_i) = wx_i + b, 令 f(x_i)  y_i
$$

在线性回归中, 最小二乘法就是试图找到一条直线, 使所有样本到直线上的欧几里得距离之和最小:

$$
(w^*, b^*) = arg_{(w, b)}min(\sum^m_{i=1}(f(x_i)-y_i))^2 = arg_{(w,b)}min(\sum^m_{i=1}(y_i-wx_i-b))^2
$$

## 最小二乘法求解线性回归

求解 $w$ 和 $b$, 使 $E_{(w,b)} = \sum^m_{i=0}(y_i - wx_i - b)^2$ 最小, 称为线性回归模型的**最小二乘参数估计**.

将 $E_{(w,b)}$ 分别对 $w$ 和 $b$ 求偏导, 可以得到:

$$
\frac {\partial E(w,b)}{\partial w} = 2 (w \sum^m_{i=1}x_i^2 - \sum^m_{i=1}(y_i-b)x_i)\\
\frac {\partial E(w,b)}{\partial b} = 2 (mb - \sum^m_{i=1}(y_i-wx_i))
$$

令偏导都为0, 可以得到:

$$
w = \frac {\sum_{i=1}^my_i(x_i - \bar{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2}\\
b = \frac{1}{m}\sum_{i=1}^m(y_i - wx_i)\\
\bar{x} = \frac{1}{m}\sum^m_{i=1}x_i
$$

## 梯度下降法求解线性回归

定义损失函数:

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^m(h_\theta(x^i) - y^i)
$$

这是均方误差损失函数, 用于衡量模型预测值 $h_\theta(x^i)$ 与真实值 $y^i$ 之间的差异, $m$ 是样本量的批次大小. 我们的目标是找到合适的参数 $\theta$, 使得 $J(\theta)$ 最小.

模型函数:

$$
h_\theta(x) = X^T\theta = \sum_{k=0}^n \theta_k x_k
$$

这个函数代表了线性回归模型, 表示对输入特征 $x$ 的预测, $X^T\theta$ 是矩阵式表达, $n$ 是特征的维度, 而 $\theta_k$ 是模型的参数.

梯度下降的更新规则如下:

$$
\theta := \theta - \alpha \cdot \frac {\partial J(\theta)}{\partial \theta}
$$

其中, $\alpha$ 表示学习率, 其控制了每次更新的步长, $\frac {\partial J(\theta)}{\partial \theta}$ 是损失函数 $J(\theta)$ 对参数 $\theta$ 的梯度, 表示 $J(\theta)$ 在参数空间中的变化方向和速率.

通过不断迭代更新 $\theta$, 使 $J(\theta)$ 逐渐减小, 最终收敛到最小值.

梯度计算:

$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial}{\partial \theta_j}\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^i) - y^i)^2 = \frac{2}{m}\sum_{i=1}^m(h_\theta(x^i) - y^i) \cdot x_j^i
$$

通过计算可以得到损失函数对参数 $\theta_j$ 的偏导数, 得到梯度的一个分量, 通过逐个计算每个参数的偏导和损失, **将多元线性回归退化为一元线性回归求解**.

## 最小二乘和梯度下降的区别

- 损失函数: 梯度下降可以选取其他损失函数, 而最小二乘一定是使用均方损失.

- 实现方法: 最小二乘是直接求导找出全局最小值, 而梯度下降是通过不断迭代优化来逼近最小值.

- 效果: 最小二乘法找到的一定是全局最小值, 但计算繁琐, 复杂情况下未必有解. 而梯度下降计算简单, 但找到的可能是局部最小值, 只有在目标函数的凸函数时才是全局最小值, 其在最小值点附近时收敛速度会变慢, 且对初始点的选择极为敏感.
