---
layout: post
toc: true
title: "旋转位置编码 (RoPE), 原理与应用"
categories: NLP
tags: [NLP, DeepLearning]
author:
  - vortezwohl
  - 吴子豪
---
旋转位置编码（Rotary Position Embedding, RoPE）是一种新颖的位置编码方法，旨在更有效地将位置信息集成到 Transformer 模型中。它通过旋转查询（Query）和键（Key）向量来注入绝对位置信息，同时巧妙地在自注意力机制中实现了相对位置编码。RoPE 最初由 Jianlin Su 等人在论文 [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) 中提出。文中详细阐述了 RoPE 的数学原理、实现方式以及其在各种自然语言处理任务中的优势。

## 传统位置编码的局限性

在 RoPE 之前, 位置编码的普遍策略是**正余弦位置编码 (Sinusoidal Position Encoding)**. 对于一个给定位置 $pos$ 和维度索引 $i$ (其中 $i$ 从 0 到 $d - 1$, $d$ 是嵌入的维度), 位置编码 $PE_{(pos, i)}$ 的计算方式如下: 

$$
PE_{(pos, 2k)} = sin(\frac{pos}{10000^{\frac{2k}{d}}})
$$

$$
PE_{(pos, 2k+1)} = cos(\frac{pos}{10000^{\frac{2k}{d}}})
$$

其中, $pos$ 是 token 在序列中的位置, $k$ 是维度索引的一半, 因为正弦和余弦成对出现, $d$ 是位置编码的维度 (通常和嵌入维度相同).

**正余弦位置编码的局限性主要体现在以下方面**:

1. **缺乏相对位置的直接表达**: 正余弦位置编码通过绝对位置计算每个位置的编码值，虽然在一定程度上可以让模型学习到位置相关的信息，但并没有直接体现出不同位置之间的相对关系。在处理一些需要捕捉相对位置信息的任务时，模型可能需要通过复杂的学习过程才能间接获取这些信息，这增加了学习的难度。

2. **对长序列的适应性不足**: 随着序列长度的增加，正余弦位置编码的性能会逐渐下降。这是因为正余弦函数的周期性特点，当序列长度超出一定范围后，不同位置的编码值可能会变得过于相似，导致模型难以区分不同位置的信息，从而影响模型对长序列的处理能力。

3. **缺乏灵活性**: 正余弦位置编码是基于固定的数学式生成的，一旦确定了维度和序列长度，编码的形式就固定下来。在面对不同长度或不同类型的输入序列时，难以灵活地调整编码方式以更好地适应任务需求。

## RoPE 的创新性

1. **长序列处理能力**: 与传统的正余弦位置编码相比，RoPE 在处理长序列时表现更为出色。由于其旋转矩阵的设计，RoPE 能够更好地保持不同位置之间的信息区分度，即使在序列长度增加的情况下，也能让模型有效地捕捉到长距离依赖关系。这使得基于 RoPE 的 Transformer 模型在处理长文本任务，如文档级别的自然语言处理任务中具有显著优势。

2. **相对位置编码的有效性**: RoPE 通过旋转操作直接在自注意力机制中引入相对位置编码，使得模型能够更直接地学习到序列中元素之间的相对位置关系。这种直接的相对位置编码方式相比于传统编码方法需要模型间接学习相对位置信息，能够提高模型的学习效率和性能，特别是在一些对相对位置敏感的任务中，如语法分析、语义角色标注等。

3. **灵活性**: RoPE 在处理不同长度的序列时具有更高的灵活性。由于其基于旋转矩阵的编码方式，不需要像传统编码那样预先确定固定的序列长度。在实际应用中，无论是短序列还是长序列输入，RoPE 都能够动态地为每个位置生成合适的编码，从而更好地适应各种任务需求。

## RoPE 的数学原理: 旋转矩阵

**RoPE 利用旋转矩阵对 Query 和 Key 向量进行旋转**来编码位置信息, 给定位置 $pos$ 与向量 $[x, y]^T$, RoPE 定义的旋转矩阵 $\textbf{R}_{pos}$ 如下:
$$
\begin{bmatrix}
x' \\
y'
\end{bmatrix}
=
\begin{bmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$

> 旋转矩阵用于对向量进行旋转操作, 对于二维向量 $(x, y)$, 旋转 $\theta$ 度后的向量 $(x', y')$ 可以通过该旋转矩阵计算: $\begin{bmatrix}\cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)\end{bmatrix} \begin{bmatrix}x \\ y\end{bmatrix}$, 其中 $\theta$ 是旋转的角度.

在 RoPE 中, $pos$ 是位置索引, $\theta$ 由位置 $pos$ 决定, 具体为 $\theta = \frac {pos}{10000 ^ {\frac {2k}{d}}}$, $k$ 是维度索引 (与正余弦位置编码中的 $k$ 类似). 

对于**高维向量**, RoPE 将输入向量分为偶数维度和奇数维度两部分: $x_{even} = [x_1, x_3, ..., x_{d-1}]$, $x_{odd} = [x_2, x_4, ..., x_d]$, 并对偶数维度和奇数维度部分分别应用旋转矩阵:

$$
x'_{even} = \textbf{R}_{pos} \cdot x_{even} \\
x'_{odd} = \textbf{R}_{pos} \cdot x_{odd}
$$

最后, 将 $x'_{even}$ $x'_{odd}$ 进行**拼接 (concatenation)**, 得到最终的旋转位置编码 $x'$.

## RoPE 的实现 (基于 Torch)

[*基于 Torch 实现 RoPE*](https://vortezwohl.github.io/dl/2025/07/28/BuildTransformerEncoderFromScratch.html#%E5%9F%BA%E4%BA%8E-torch-%E5%AE%9E%E7%8E%B0-rope-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9D%97)

## RoPE 在注意力计算中的作用

RoPE 的一个显著优点是, 它不但编码了绝对位置信息, 还在注意力计算中自然地引入了相对位置信息. 在注意力计算中, Query 和 Key 在经过 RoPE 旋转后, 它们之间的点积操作就能够捕捉到相对位置关系: 

$$
A = \text{softmax}\left(\frac{Q'K'^T}{\sqrt{d_k}}\right )
$$

其中, Q' 和 K' 是 Query 和 Key 的旋转向量 (所构成的矩阵). 由于旋转操作, 不同位置的 token 的 Query 和 Key 会反映出它们的相对位置, 从而使得模型能够有效地捕捉序列中的相对位置关系. 

## RoPE 的变体与研究进展 (2025年5月)

在处理超长序列时，RoPE 面临分布外（OOD）问题，即当序列长度超出预训练的上下文窗口时，性能会下降。为了解决这一问题，研究人员提出了如 PI、ABF、NTK 和 YaRN 等变体。这些变体通过不同的缩放机制来增强模型在超长序列上的性能[[1]](https://openreview.net/forum?id=Y6yz85kqL9). 此外，还有研究从理论层面深入探讨 RoPE 的性质和局限性。例如，有研究提出了一个基于李群和李代数理论的系统数学框架，用于统一和解释现有的 RoPE 设计，并为其向新模态和任务的扩展提供了理论基础[[2]](https://export-test.arxiv.org/abs/2504.06308v1).

> 需要注意的是, 正余弦位置编码作用于词嵌入层, 在词嵌入表示中注入位置信息; 而旋转位置编码则直接作用于注意力层, 在 Query 和 Key 向量中注入位置信息.