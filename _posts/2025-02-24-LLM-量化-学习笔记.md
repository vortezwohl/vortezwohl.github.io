---
layout: post
toc: true
title: "大模型权重量化学习笔记"
categories: Note
tags: [AI, LLM]
author:
  - Vortez Wohl
---

# 大模型权重量化

大模型量化技术是一种用于优化深度学习模型的技术，通过降低模型参数和激活值的精度，将高精度的浮点数（如32位浮点数FP32）转换为低精度的表示形式（如8位整数INT8或4位整数INT4），从而减少模型的存储需求和计算复杂度.

量化技术的核心是将连续的浮点数映射到离散的量化值上，通过这种方式，在损失少量精度的前提下，显著减少模型的内存占用，并提高推理速度.

大模型量化主要分为以下几种方法:

- PTQ (训练后量化, Post-Train Quantization)

    在模型训练完成后进行量化, 不需要额外的训练步骤. PTQ 又分为动态量化和静态量化

- QAT (量化感知训练, Quantization-Aware Training)

    在模型训练过程中插入伪量化算子, 模拟量化操作, 使模型适应量化带来的影响, 从而提高量化后的性能

- QAF (量化感知微调, Quantization-Aware Fine-tuning)

    结合量化和微调的优势, 适用于需要在压缩和性能之间取得平衡的场景

量化可以按不同的粒度进行, 例如逐层量化(每层独立莲花), 逐通道量化(对权重的每个通道分别量化), 和全局量化(使用统一的量化参数对整个模型进行量化)

## 原理简述

浮点数量化为整数的过程是将高精度的浮点数（如32位浮点数FP32）转换为低精度的整数（如8位整数INT8）的过程。这一过程通常涉及以下几个关键步骤：

1. 确定量化范围

    对于 INT8, 有符号整数的范围是 [-128, 127], 无符号整数的范围是 [0, 255].

2. 计算缩放因子 (Scale Factor)

    缩放因子用于将浮点数的范围映射到整数的范围. 计算步骤如下:

    - 找出原数据的最大绝对值 (max_abs_value)

    - 根据目标整数范围的最大值 (max_int_value) 计算缩放因子

        $$
        scale = \frac {max\_int\_value} {max\_abs\_value}
        $$

3. 量化计算

    量化值的算法如下:

    $$
    quantized\_value = round(float\_value \times scale)
    $$

4. 裁切 (Clipping)

    量化后的整数需要确保在目标整数范围内, 如果超出范围, 则需要进行裁切

> 在量化处理上, 可能出现对称量化和非对称量化两种情况, 对称量化中, 0 点将被继续映射为 0 点, 二非对称量化中, 需要额外计算 0 点

对于量化后的权重值, 我们也可以将其反量化为浮点数:

$$
scale = \frac {127} {max\_int\_value}
$$

$$
dequantized\_value = \frac {quantized\_value} {scale}
$$

> 假设采用有符号 INT8 量化, 权重取值范围为 [-128, 127]

## LLaMA Factory 提供的量化方案

- bitsandbyes

    这是一个 Python SDK, 专门用于神经网络模型的量化, 它支持多种量化精度, 包括 INT8, INT4 等

- HQQ (High-Quality Quantization)

    这是一种旨在保持模型精度的量化技术, 它通过优化量化过程, 减少量化误差, 从而尽可能地保持模型的精度. HQQ 通过分析模型的权重分布, 选择最优的缩放因子和零点, 以最小化量化误差. 同时对每一层的权重和激活值进行独立量化, 以保持模型量化后的精度

- EETQ (Easy and Efficient Quantization for Transformers)

    这是一种专为 Transformer 模型设计的量化库，它支持 NVIDIA GPU上的 INT8 量化，并通过 per-channel 量化策略保持了几乎无损的精度

    EETQ利用Flash-Attention V2对注意力机制进行优化，通过对attention层的精心调优，能够显著改善模型的推理性能

    > 在 Transformer 模型中，"channel"通常指的是模型中的一个维度，特别是在多头自注意力（Multi-Head Attention）机制中。在这种上下文中，"channel"并不是指数据的物理通道，而是指数据在模型内部的一个抽象维度。例如，在多头自注意力中，输入数据被分割成多个"头"（heads），每个头可以看作是一个"channel"，它们并行地处理数据，然后结果被合并。