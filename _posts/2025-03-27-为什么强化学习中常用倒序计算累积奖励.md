---
layout: post
toc: false
title: "RL中的奖励折扣机制"
categories: RL
tags: [RL, PG, Math]
author:
  - vortezwohl
  - 吴子豪
---
在学习并实现强化学习算法中, 我发现累积奖励的计算通常是倒序计算的. 究其原因, 我明白了, 这么计算其实是一种递推算法, 也和其**奖励折扣机制**有关.

## 奖励折扣机制

强化学习例如[策略梯度](https://vortezwohl.github.io/rl/2025/03/26/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95.html)算法中, 奖励是折扣的. 折扣因子 $\gamma$ 用于衡量未来奖励对当前决策的重要性. 其数学表示如下:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...
$$

其中:

- $G_t$: 表示从时间步 $t$ 开始的累积奖励

- $R_{t+1}$: 于时间步 $t+1$ 获得的奖励

- $\gamma$: 折扣因子, 取值范围 $\gamma \in [0, 1]$

折扣因子 $\gamma$ 则是算法的核心超参数, $\gamma$ 越小, 则在指数 $n$ 不断增加的情况下, $\gamma ^ n$ 越来越小, 这表示未来的奖励被 $\gamma$ 权重系数缩小到越来越小的值, 即权重更小或影响更小.

## 程序实现中, 奖励为什么要反向计算

假设智能体轨迹 $\tau$ 共涉及 4 个时间步, 在时间步 1 的累积奖励计算可以表示为如下形式:

$$
G_1 = R_2 + \gamma \cdot (R_3 + \gamma \cdot R_4) \\
G_2 = R_3 + \gamma \cdot R_4 \\
G_3 = R_4
$$

可以得出:

$$
G_1 = R_2 + \gamma \cdot G_2 \\
G_2 = R_3 + \gamma \cdot G_3
$$

所以上述计算过程可以用递推式表示:

$$
G_{t-1} = R_t + \gamma \cdot G_t
$$

在程序上可以如此实现:

```python
cumulative_reward = 0
for reward in reversed(episode_rewards):
    cumulative_reward = reward + gamma * cumulative_reward
```