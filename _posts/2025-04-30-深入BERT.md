---
layout: post
toc: true
title: "深入 BERT 模型与基于 BERT 的自然语言理解任务"
categories: NLP
tags: [NLP, DeepLearning]
author:
  - vortezwohl
  - 吴子豪
---
BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 的深度学习模型，[由 Google 于 2018 年提出](https://arxiv.org/abs/1810.04805)。其核心优势在于使用双向上下文来预训练模型，能够更好地理解和捕捉句子中的语境信息，从而在多种自然语言处理任务中取得了卓越的性能。与之前的模型不同，BERT 表示在所有层中都共同基于左右上下文，这使得它能够在处理输入序列时同时考虑到所有位置的上下文信息。

## BERT 模型结构

```
Input: [CLS] token1 [SEP] token2 [PAD] ... [PAD] [PAD] [SEP]
       ↓
Embedding Layer:
Token Embeddings                   Segment Embeddings     Position Embeddings
       ↓                                   ↓                       ↓
       +-----------------------------------+-----------------------+
       |                                                           |
       ↓                                                           ↓
Transformer Encoder Layer 1:                                       
Multi-Head Self-Attention → Add & Norm → Feed-Forward → Add & Norm |
       ↓                                                           ↓
Transformer Encoder Layer 2:                                       
Multi-Head Self-Attention → Add & Norm → Feed-Forward → Add & Norm |
       ↓                                                           ↓
       ...                                                         |
       ↓                                                           ↓
Transformer Encoder Layer N:
Multi-Head Self-Attention → Add & Norm → Feed-Forward → Add & Norm
       ↓
Output: Hidden states for each token
       ↓
Task-specific layers (e.g., classification, QA)
```

- **输入层**: 输入 Tokens 经过预处理后，将转化为三个嵌入向量：Token Embeddings（词元嵌入）、Segment Embeddings（句子嵌入）和 Position Embeddings（位置嵌入）。Token Embeddings 是将每个单词映射到一个固定维度的向量；Segment Embeddings 用于区分两个句子；Position Embeddings 是单词在句子中的位置信息，通过位置嵌入让模型感知到单词的顺序。

- **编码器层**: Transformer 编码器是 BERT 模型的核心组成部分，它由多个相同的层堆叠而成。每一层包含两个主要的子层：多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。

    - **多头注意力模块**: 该模块用于捕捉输入序列中不同单词之间的关系。它将输入向量变换为查询（Query）、键（Key）和值（Value）向量，然后计算查询和键之间的点积注意力，得到每个单词与其它单词的相关性权重，再用这些权重对值向量加权求和，得到输出。多头注意力机制是指将输入向量映射到多个不同的查询、键和值空间，分别计算注意力，然后将结果拼接起来，再通过一个线性变换得到最终输出，这样可以使模型能够关注到不同位置之间的不同关系。

    - **前馈神经网络**: 该子层对每个位置的向量进行独立的非线性变换。它由两个线性变换中间夹一层激活函数（通常是 ReLU）组成，能够增加模型的表达能力，使模型能够学习到更复杂的特征。

- **输出层**: Transformer 编码器的输出向量可以用于各种下游任务。例如在句子分类任务中，通常将第一个单词（[CLS] 标记）的输出向量作为整个句子的表示，然后在其后面添加一个分类层进行分类。

## BERT 预训练方法

BERT 的训练采用无监督学习方法，通过两个主要任务来学习词汇表示和上下文信息：掩码语言模型（Masked Language Model，MLM）和下一句预测（Next Sentence Prediction，NSP）。

- **MLM, 掩码语言模型**: 在训练过程中，随机遮蔽输入文本中的一些 token，然后让模型预测这些被遮蔽的 token。具体步骤为：对于输入文本中的每个句子，随机选择 15% 的 token 进行遮蔽，其中 80% 的被遮蔽 token 被替换成一个特殊的 [MASK] 标记，10% 的 token 被替换成其他随机的 token，而剩下的 10% 则保持原样。模型的目标是根据上下文预测被遮蔽的 token，在训练期间，通过比较模型输出和实际被遮蔽的 token 来计算损失，通过反向传播和优化算法来更新模型参数。

- **WWM, 全词掩码训练方法**: 全词掩码（Whole Word Masking，wwm）策略是针对中文等语言特性进行改进的一种掩码方法。在中文条件下，由于 WordPiece tokenizer 不会将词语拆分成小片段，因此采用传统的中文分词工具将文本分割成多个词语，然后以整个词语为单位进行掩码，而不是随机选择 WordPiece tokens 来掩码。这样能更符合语言习惯，使模型在预训练阶段就聚焦于对整个词语语义的学习和理解，为后续的微调任务提供更好的基础。

- **NSP, 下一句预测**: 该任务旨在让模型学习理解句子之间的关系。在训练过程中，随机选择一些相邻的句子对和两个不相邻的句子对，模型需要判断哪些句子对是连续的、有逻辑关系的，而哪些句子对是不相关的。通过这个任务，BERT 能够学习捕捉句子之间的语义关系，从而提高对文本中句子级别信息的理解能力。

## 使用 BERT 对段落进行编码

在 BERT 中，输入序列的第一个 token 是一个特殊的 [CLS] 标记，其对应的输出向量被认为包含了整个输入序列的综合信息，因此常被用作文本嵌入表示。具体步骤如下：

- **输入文本处理**: 将待嵌入的文本进行分词, 添加 [CLS] [PAD] [SEP] 等标记, 并转为张量形式.

- **模型前向传播**: 将处理后的输入序列输入到 BERT 中进行前向传播计算, 模型会输出每个 Token 对应的隐藏状态向量.

- **获取嵌入向量**: 取出输出序列中, [CLS] 标记位置的向量, 作为该文本的嵌入表示, [CLS] 标记经过 BERT 的多层编码器处理, 其融合了整个输入序列的总信息, 能够较好地反映文本的语义特征, 可用于后续的文本分类等任务.
