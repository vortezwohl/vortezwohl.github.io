---
layout: post
toc: true
title: "L1 范数与 L2 范数"
categories: Math
tags: [MachineLearning, DeepLearning, Math]
author:
  - vortezwohl
  - 吴子豪
---

L1 范数（曼哈顿范数）是向量元素绝对值之和，具有稀疏性，在特征选择及稀疏信号恢复等场景有用；L2 范数（欧几里得范数）是向量元素平方和的平方根，平滑且唯一解，常用于机器学习正则化、优化问题以及距离度量等场合。

## L1 范数

- **定义**: L1 范数也称为曼哈顿范数或绝对值范数, 是向量中各个元素的绝对值之和

- **数学表示**: 对于一个实数向量 $x = (x_1, x_2, x_3, ..., x_n)$, 其 L1 范数被定义为 $\|x\|_1 = \sum_{i=0}^n \|x_i\|$

- **性质**: 

    - L1 范数始终的非负的, 即 $\|x\|_1 \ge 0$

    - 对于任何标量 $c$ 和向量 $x$, 有 $\|c \cdot x\|_1 = \|c\| \cdot \|x\|_1$

    - 对于任意两个向量 $x$ 和 $y$, 有 $\|x + y\|_1 \le \|x\|_1 + \|y\|_1$

    - L1 范数倾向于产生稀疏解, 即解中的许多元素为 0. 因为在优化问题中, L1 范数会促使权值不断向 0 靠拢, 从而使不重要的特征对应的权值变为 0, 实现特征的选择

    - L1 范数是一个凸函数, 这意味着对于任何两个向量 $x$ 和 $y$ 以及 $0 \le t \le 1$, 都有 $\|t \cdot x + (1 - t) \cdot y\|_1 \le t\|x\|_1 + (1 - t)\|y\|_1$, 凸函数性质使其能够在优化过程中找到全局最优解

    - L1 范数的单位球是一个 $n$ 维超立方体, 其边长为 2

- **应用场景**:

    - 鲁棒性模型训练/特征选择: 由于 L1 范数对异常值具有一定的鲁棒性，因此在训练机器学习模型时，若数据中存在噪声或异常值，使用 L1 范数作为损失函数的一部分，可以使模型对这些异常数据不那么敏感。

    - 稀疏信号恢复: 在信号处理领域，如压缩感知中，通过 L1 范数可以实现对稀疏信号的精确恢复，仅需少量的测量值就能重建出原始信号。

## L2 范数

- **定义**: L2 范数也叫欧几里得范数或平方和范数, 是向量中各元素的平方和的平方根

- **数学表示**: 对于一个实数向量 $x$, 其 L2 范数定义为 $\|x\|_2 = \sqrt{(x_1^2 + x_2^2 + ... + x_n^2)}$

- **性质**: 

    - L2 范数是非负的，且仅当向量的所有元素都为零时，其范数为零

    - 对于任何标量 $c$ 和向量 $x$, 有 $\|c \cdot x\|_2 = \|c\| \cdot \|x\|_2$

    - 对于任意两个向量 $x$ 和 $y$, 有 $\|x + y\|_2 \le \|x\|_2 + \|y\|_2$

    - L2 范数的平方在求导时较为方便，优化过程中容易求解极值问题，因此在机器学习和最优化领域应用广泛。其平方被称为平方模或平方范数

    - L2 范数的最小化解是唯一的，这在优化问题中具有重要意义，能够保证问题有明确的最优解

    - 在几何上，L2 范数表示向量对应点到原点的欧几里得距离, 在二维平面上，向量 $(x, y)$ 的 L2 范数就是该点到原点的距离

- **应用场景**:

    - 机器学习: L2 范数常被用作正则化项，如岭回归（Ridge Regression）中，通过在损失函数中添加 L2 范数项，可以限制模型参数的大小，防止过拟合，提高模型的泛化能力

    - 优化问题: 在优化问题中，L2 范数可以作为目标函数或约束条件，用于衡量解的质量或可行性。例如在最小二乘法中，通过最小化残差的 L2 范数来求解线性方程组的最佳近似解

    - 距离度量: 由于 L2 范数具有直观的几何意义，因此在计算两个向量或点之间的距离时，常使用 L2 范数, 也就是欧几里得距离