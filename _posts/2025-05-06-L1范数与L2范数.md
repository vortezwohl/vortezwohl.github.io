---
layout: post
toc: true
title: "深度学习中的权重正则化方法: Lasso/Ridge/ElasticNet"
categories: Math
tags: [MachineLearning, DeepLearning, Math]
author:
  - vortezwohl
  - 吴子豪
---

深度学习模型，凭借其强大的特征学习和复杂模式识别能力，在诸多领域取得了显著成就。然而，这些模型的复杂性也使其在训练数据量不足或噪声干扰较大时，极易出现过拟合（Overfitting）现象。过拟合指的是模型在训练集上表现优异，但在未见过的测试集上性能显著下降，这表明模型学习到了训练数据中的噪声和特有模式，而非普适的规律。为了缓解过拟合，提升模型的泛化能力，正则化（Regularization）技术应运而生。正则化通过向模型的损失函数中引入额外的惩罚项，对模型的复杂度进行约束，引导模型学习更简单、更平滑的参数。在众多正则化方法中，L1 范数正则化（Lasso 回归）、L2 范数正则化（Ridge 回归）以及它们的组合 Elastic Net 正则化，是最为经典和广泛应用的权重正则化技术。

# 基本知识

## L1 范数

- **定义**: L1 范数也称为**曼哈顿范数**或绝对值范数, 是向量中各个元素的绝对值之和

- **数学表示**: 对于一个实数向量 $x = (x_1, x_2, x_3, ..., x_n)$, 其 L1 范数被定义如下: 

$$
\|x\|_1 = \sum_{i=0}^n |x_i|
$$

- **性质**: 

    - L1 范数始终的非负的, 即 $\|x\|_1 \ge 0$

    - 对于任何标量 $c$ 和向量 $x$, 有 $\|c \cdot x\|_1 = \|c\| \cdot \|x\|_1$

    - 对于任意两个向量 $x$ 和 $y$, 有 $\|x + y\|_1 \le \|x\|_1 + \|y\|_1$

    - L1 范数倾向于产生稀疏解, 即解中的许多元素为 0. 因为在优化问题中, L1 范数会促使权值不断向 0 靠拢, 从而使不重要的特征对应的权值变为 0, 实现特征的选择

    - L1 范数是一个**凸函数**, 这意味着对于任何两个向量 $x$ 和 $y$ 以及 $0 \le t \le 1$, 都有 $\|t \cdot x + (1 - t) \cdot y\|_1 \le t\|x\|_1 + (1 - t)\|y\|_1$, 凸函数性质使其能够在优化过程中找到全局最优解

        > 凸函数 (Convex function) 是一种函数类型，它描述了函数图像的形状特性。简单来说，如果一个函数的图像在任意两点之间的线段都在函数图像的上方或刚好与图像重合，那么这个函数就是凸函数。举个例子，考虑常见的二次函数 $y=x^2$，其图像是开口向上的抛物线。当你在抛物线上取任意两个点，并连接这两个点形成一条线段时，这条线段会位于抛物线的上方或与抛物线重合。这说明 $y=x^2$ 是一个凸函数。\
        > \
        > 凸函数有以下重要性质：
        > -  凸函数在其定义域内导数是单调递增的。这意味着随着输入值的增加，函数的斜率会变得越来越陡峭或保持不变。
        > - 凸函数在其定义域内如果有极小值，那么这个极小值是全局的。也就是说，如果找到一个局部极小值点，那么它就是整个函数的最小点。
        > - 凸函数的非负线性组合仍然是凸函数。比如，如果有两个凸函数 $f(x)$ 和 $g(x)$，以及非负系数 $a$ 和 $b$，那么函数 $h(x)=a \cdot f(x)+b \cdot g(x)$ 也是凸函数。

    - L1 范数的单位球是一个 $n$ 维超立方体, 其边长为 2

- **应用场景**:

    - 鲁棒性模型训练/特征选择: 由于 L1 范数对异常值具有一定的鲁棒性，因此在训练机器学习模型时，若数据中存在噪声或异常值，使用 L1 范数作为损失函数的一部分，可以使模型对这些异常数据不那么敏感。

    - 稀疏信号恢复: 在信号处理领域，如压缩感知中，通过 L1 范数可以实现对稀疏信号的精确恢复，仅需少量的测量值就能重建出原始信号。

## L2 范数

- **定义**: L2 范数也叫**欧几里得范数**或平方和范数, 是向量中各元素的平方和的平方根

- **数学表示**: 对于一个实数向量 $x$, 其 L2 范数定义如下: 

$$
\|x\|_2 = \sqrt{(x_1^2 + x_2^2 + ... + x_n^2)}
$$

- **性质**: 

    - L2 范数是非负的，且仅当向量的所有元素都为零时，其范数为零

    - 对于任何标量 $c$ 和向量 $x$, 有 $\|c \cdot x\|_2 = \|c\| \cdot \|x\|_2$

    - 对于任意两个向量 $x$ 和 $y$, 有 $\|x + y\|_2 \le \|x\|_2 + \|y\|_2$

    - L2 范数的平方在求导时较为方便，优化过程中容易求解极值问题，因此在机器学习和最优化领域应用广泛。其平方被称为平方模或平方范数

    - L2 范数的最小化解是唯一的，这在优化问题中具有重要意义，能够保证问题有明确的最优解

    - 在几何上，L2 范数表示向量对应点到原点的欧几里得距离, 在二维平面上，向量 $(x, y)$ 的 L2 范数就是该点到原点的距离

- **应用场景**:

    - 机器学习: L2 范数常被用作正则化项，如岭回归（Ridge Regression）中，通过在损失函数中添加 L2 范数项，可以限制模型参数的大小，防止过拟合，提高模型的泛化能力

    - 优化问题: 在优化问题中，L2 范数可以作为目标函数或约束条件，用于衡量解的质量或可行性。例如在最小二乘法中，通过最小化残差的 L2 范数来求解线性方程组的最佳近似解

    - 距离度量: 由于 L2 范数具有直观的几何意义，因此在计算两个向量或点之间的距离时，常使用 L2 范数, 也就是欧几里得距离

<!-- # L1 范数与 L2 范数在深度学习中的应用 -->




