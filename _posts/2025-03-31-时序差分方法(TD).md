---
layout: post
toc: false
title: "广义优势估计方法 (GAE)"
categories: RL
tags: [RL]
author:
  - vortezwohl
  - 吴子豪
---

TD（Temporal Difference）方法是一种强化学习中的值函数估计方法，结合了蒙特卡洛（Monte Carlo, MC）方法和动态规划（Dynamic Programming, DP）方法的特点。它的核心思想是通过比较当前时刻的奖励和未来的值函数估计来更新当前的值函数。

## 原理

TD 方法的关键在于 TD 残差（TD Error），它表示当前时刻的奖励与未来值函数估计之间的差异。通过 TD 残差，TD 方法能够在不等待完整回合结束的情况下逐步更新值函数，从而在偏差和方差之间找到平衡。

### TD 残差的数学表示:

$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

其中:

- $r_t$ 是时间步 $t$ 的即时奖励

- $\gamma$ 是折扣因子, 用于控制未来回报的权重

- $V(s_{t+1})$ 是时间步 $t+1$ 的[状态价值函数](https://vortezwohl.github.io/rl/2025/03/26/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0.html)

- $V(s_t)$ 是时间步 $t$ 的状态价值函数

### 价值函数更新:

TD 方法中价值函数更新计算如下:

$$
V(s_t) = V(s_t) + \alpha \delta_t
$$

其中:

- $\alpha$ 是学习率, 用于控制更新步长

- $\delta_t$ 是 TD 残差, 表示当前时刻奖励与价值函数给出的未来奖励估计的差异

### TD 算法步骤:

1. 初始化 $V(s)$ 和学习率 $\alpha$

2. 与环境交互, 采样一个轨迹, 记录状态 $s_t$ 动作 $a_t$ 奖励 $r_t$ 和下一状态 $s_{t+1}$

3. 计算 [TD 残差](#td-残差的数学表示)

4. 更新价值函数

5. 重复以上过程, 直到价值函数收敛

