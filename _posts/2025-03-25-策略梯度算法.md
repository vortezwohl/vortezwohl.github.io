---
layout: post
toc: true
title: "现代强化学习-策略梯度算法学习"
categories: Note
tags: [RL, PG]
author:
  - Vortez Wohl
---
策略梯度算法（Policy Gradient, PG）是一类直接对策略进行优化的强化学习算法。其核心思想是通过梯度上升法来调整策略参数，使得策略在选择高奖励动作时的概率增加，从而最大化累积奖励。

## PG 算法的目标函数

策略梯度算法的目标是最大化期望累积奖励, 那么我们定义 $J(\theta)$ 表示策略网络参数为 $\theta$ 时的累计奖励. 具体来说, 它是**在概率分布 $p_\theta$ 下, 对所有可能的轨迹 $\tau$ 的累计奖励 $\sum_{t=0}^T r(s_t, a_t)$ 求期望**, 其数学表示如下:

$$
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^T r(s_t, a_t) \right]
$$

其中，$\tau = (s_0, a_0, s_1, a_1, ..., s_T)$ 表示一个轨迹，$p_\theta(\tau)$ 是在策略 $\pi_\theta$ 下轨迹 $\tau$ 的概率.

根据策略梯度定理，策略参数的梯度可以表示为：
$$
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta} \left[ Q^\pi(s, a) \nabla_\theta \log \pi_\theta(a|s) \right]
$$
其中，$Q^\pi(s, a)$ 是在策略 $\pi$ 下的 action-value 函数，$d^\pi(s)$ 是在策略 $\pi$ 下的状态分布。

策略梯度算法的更新规则基于策略梯度定理，通过梯度上升来更新策略参数：
$$
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k)
$$
其中，$\alpha$ 是学习率。

在实际应用中，常用的REINFORCE算法是一种蒙特卡洛策略梯度方法，其更新规则为：
$$
\theta_{k+1} = \theta_k + \alpha \sum_{t=0}^T \left( \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right) \nabla_\theta \log \pi_\theta(a_t|s_t)
$$
这里，$\sum_{t'=t}^T r(s_{t'}, a_{t'})$ 是从时间步 $t$ 开始的累积奖励。

...待续