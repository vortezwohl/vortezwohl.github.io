---
layout: post
toc: true
title: "现代强化学习-策略梯度算法学习"
categories: RL
tags: [RL, PG]
author:
  - Vortez Wohl
---
策略梯度算法（Policy Gradient, PG）是一类直接对策略进行优化的强化学习算法。其核心思想是通过梯度上升法来调整策略参数，使得策略在选择高奖励动作时的概率增加，从而最大化累积奖励。

## PG 算法的优化目标（目标函数）

策略梯度算法的目标是最大化期望累积奖励, 那么我们定义 $J(\theta)$ 表示策略网络参数为 $\theta$ 时的累计奖励. 具体来说, 它是**在概率分布 $p_\theta$ 下, 对所有可能的轨迹 $\tau$ 的累计奖励 $\sum_{t=0}^T r(s_t, a_t)$ 求期望**, 其数学表示如下:

$$
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^T r(s_t, a_t) \right]
$$

其中，$\tau = (s_0, a_0, s_1, a_1, ..., s_T)$ 表示一个轨迹，$p_\theta(\tau)$ 是在策略 $\pi_\theta$ 下轨迹 $\tau$ 的概率.

### 详细地说：

- $J(\theta)$: 表示策略参数为 $\theta$ 时的累计奖励，是我们希望最大化的目标函数

- $\mathbb{E}$: 表示数学期望，指针对所有可能轨迹 $\tau$ 计算的期望值

- $\tau \sim p_\theta(\tau)$: 表示轨迹 $\tau$ 是从概率分布 $p_{\theta}(\tau)$ 中采样的

- $p_\theta(\tau)$：表示在策略 $\pi_\theta$ 下，轨迹 $\tau$ 出现的概率，策略 $\pi_\theta$ 决定了每个状态 $s_t$ 下选择动作 $a_t$ 的概率，从而影响整个轨迹的概率分布

- $\sum^T_{t=0}r(s_t,a_t)$：表示轨迹 $\tau$ 中从时间步 $t=0$ 到 $t=T$ 的累计奖励，其中 $r(s_t,a_t)$是在状态 $s_t$ 下执行动作 $a_t$ 所获得的奖励

该公式的意义是，在策略 $\pi_\theta$ 下，我们考虑所有可能的轨迹 $\tau$，每个轨迹的概率是 $p_\theta(\tau)$，然后对这些轨迹的累计奖励求期望，得到一个期望累计奖励 $J(\theta)$，PG 的目标就是通过更新策略参数 $\theta$，最大化期望累计奖励

### 参数更新：

- 策略梯度定理

  PG 参数优化基于一个重要的理论基础，即**策略梯度定理**，该定理表明，策略参数 $\theta$ 的梯度 $\nabla_\theta J(\theta)$ 可以表示为在策略 $\pi_\theta$ 下，状态-动作对 $(s,a)$ 的期望优势函数与策略梯度的乘积，其数学表示如下：

  $$
  \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \cdot Q^{\pi_{\theta}}(s_t, a_t) \right]
  $$

  其中：

  - $\nabla_\theta J(\theta)$：表示策略参数 $\theta$ 的梯度，即 $J(\theta)$ 对 $
  \theta$ 的导数

  - $\mathbb{E}_{\tau \sim p_\theta(\tau)}$：表示在策略 $\pi_\theta$ 下对轨迹 $\tau$ 的期望

  - $\nabla_\theta log \pi_\theta (a_t | s_t)$：表示策略 $\pi_\theta$ 在状态 $s_t$ 下选择动作 $a_t$ 的对数概率对 $\theta$ 的梯度

  - $Q^{\pi_{\theta}}(s_t, a_t)$：表示在策略 $\pi_\theta$ 下，状态 $s_t$ 和动作 $a_t$ 的动作价值函数，即从状态 $s_t$ 执行动作 $a_t$ 后的期望累计奖励

  >...推导过程...

  策略梯度定理的直观意义是：通过调整策略参数 $\theta$，使得在每个状态 $s$ 下，选择那些能带来更高累计奖励的动作的概率增加，而选择那些带来较低累计奖励的动作的概率减少。具体来说，梯度的方向是由每个状态-动作对的对数概率梯度与该状态-动作对的期望累计奖励（即动作价值函数）的乘积决定的。

根据策略梯度定理，策略参数的梯度可以表示为：
$$
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta} \left[ Q^\pi(s, a) \nabla_\theta \log \pi_\theta(a | s) \right]
$$
其中，$Q^\pi(s, a)$ 是在策略 $\pi$ 下的 action-value 函数，$d^\pi(s)$ 是在策略 $\pi$ 下的状态分布。

策略梯度算法的更新规则基于策略梯度定理，通过梯度上升来更新策略参数：
$$
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k)
$$
其中，$\alpha$ 是学习率。

在实际应用中，常用的REINFORCE算法是一种蒙特卡洛策略梯度方法，其更新规则为：
$$
\theta_{k+1} = \theta_k + \alpha \sum_{t=0}^T \left( \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right) \nabla_\theta \log \pi_\theta(a_t | s_t)
$$
这里，$\sum_{t'=t}^T r(s_{t'}, a_{t'})$ 是从时间步 $t$ 开始的累积奖励。

> ...待续