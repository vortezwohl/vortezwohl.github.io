---
layout: post
toc: true
title: "强化学习中，什么是价值函数？"
categories: RL
tags: [RL]
author:
  - Vortez Wohl
---
价值函数 $Q^{\pi_{\theta}}(s_t, a_t)$ 确实是指在状态 $s_t$ 下执行动作 $a_t$ 后，根据策略 $\pi_{\theta}$ 继续行动所获得的期望累计奖励(累计奖励的期望)。虽然后续的轨迹尚未确定，但我们可以通过对所有可能的后续轨迹进行概率加权平均来计算这个期望值。

### 具体解释
1. **轨迹的概率分布**：在强化学习中，环境的动态（即从一个状态执行一个动作后转移到下一个状态的概率）和策略 $\pi_{\theta}$（即在每个状态选择动作的概率）共同决定了后续轨迹的概率分布。
2. **期望的计算**：对于每个可能的后续轨迹，我们可以计算其累计奖励，然后将这些累计奖励按照轨迹出现的概率进行加权求和，得到期望累计奖励。这个期望值就是价值函数 $Q^{\pi_{\theta}}(s_t, a_t)$。

### 举例说明
假设我们有一个简单的环境，状态空间为 $S = \{s_0, s_1, s_2\}$，动作空间为 $A = \{a_0, a_1\}$。在状态 $s_0$ 执行动作 $a_0$ 后，有 0.4 的概率转移到 $s_1$ 并获得奖励 1，有 0.6 的概率转移到 $s_2$ 并获得奖励 2。在 $s_1$ 和 $s_2$ 中，策略 $\pi_{\theta}$ 分别选择动作 $a_1$ 和 $a_0$，并分别获得奖励 3 和 4，然后终止。

那么，价值函数 $Q^{\pi_{\theta}}(s_0, a_0)$ 的计算如下：
- 轨迹 1：$s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1}$ 终止，奖励为 $1 + 3 = 4$，概率为 $0.4$
- 轨迹 2：$s_0 \xrightarrow{a_0} s_2 \xrightarrow{a_0}$ 终止，奖励为 $2 + 4 = 6$，概率为 $0.6$

期望累计奖励如下：

$$
Q^{\pi_{\theta}}(s_0, a_0) = 0.4 \times 4 + 0.6 \times 6 = 2 + 3 = 5
$$

在这个例子中，虽然后续轨迹尚未确定，但通过对所有可能的轨迹进行概率加权平均，计算出了价值函数的值。