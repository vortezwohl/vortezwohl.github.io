---
layout: post
toc: false
title: "强化学习中, 什么是价值函数?"
categories: RL
tags: [RL]
author:
  - Vortez Wohl
---
价值函数 $Q^{\pi_{\theta}}(s_t, a_t)$ 是指在状态 $s_t$ 下执行动作 $a_t$ 后，根据策略 $\pi_{\theta}$ 继续行动所获得的期望累计奖励(累计奖励的期望)。虽然后续的轨迹尚未确定，但我们可以通过对所有可能的后续轨迹进行概率加权平均来计算这个期望值。

# 什么是价值函数，价值函数如何计算?

### 具体解释
1. **轨迹的概率分布**：在强化学习中，环境的动态（即从一个状态执行一个动作后转移到下一个状态的概率）和策略 $\pi_{\theta}$（即在每个状态选择动作的概率）共同决定了后续轨迹的概率分布。
2. **期望的计算**：对于每个可能的后续轨迹，我们可以计算其累计奖励，然后将这些累计奖励按照轨迹出现的概率进行加权求和，得到期望累计奖励。这个期望值就是价值函数 $Q^{\pi_{\theta}}(s_t, a_t)$。

### 举例说明
假设我们有一个简单的环境，状态空间为 $S = \{s_0, s_1, s_2\}$，动作空间为 $A = \{a_0, a_1\}$。在状态 $s_0$ 执行动作 $a_0$ 后，有 0.4 的概率转移到 $s_1$ 并获得奖励 1，有 0.6 的概率转移到 $s_2$ 并获得奖励 2。在 $s_1$ 和 $s_2$ 中，策略 $\pi_{\theta}$ 分别选择动作 $a_1$ 和 $a_0$，并分别获得奖励 3 和 4，然后终止。

那么，价值函数 $Q^{\pi_{\theta}}(s_0, a_0)$ 的计算如下：
- 轨迹 1：$s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1}$ 终止，奖励为 $1 + 3 = 4$，概率为 $0.4$
- 轨迹 2：$s_0 \xrightarrow{a_0} s_2 \xrightarrow{a_0}$ 终止，奖励为 $2 + 4 = 6$，概率为 $0.6$

期望累计奖励如下：

$$
Q^{\pi_{\theta}}(s_0, a_0) = 0.4 \times 4 + 0.6 \times 6 = 2 + 3 = 5
$$

在这个例子中，虽然后续轨迹尚未确定，但通过对所有可能的轨迹进行概率加权平均，计算出了价值函数的值。

# 为什么需要价值函数，而不直接使用奖励函数?

1. ### 奖励信号的局限性
  
    奖励信号 $r(s, a)$ 是智能体在状态 $s$ 下执行动作 $a$ 时从环境中立即获得的反馈。它只能反映智能体在当前时刻的表现，而无法直接指导智能体如何在未来获得更多的累计奖励。具体来说：

    - 奖励信号是即时的，无法直接用于长期规划。
    - 奖励信号可能非常稀疏，智能体可能需要很长时间才能获得有意义的反馈。
    - 奖励信号无法直接比较不同动作的长期效果。

2. ### 价值函数的作用

    价值函数 $V^{\pi}(s)$ 或 $Q^{\pi}(s, a)$ 是对智能体在状态 $s$ 或状态-动作对 $(s, a)$ 下，按照策略 $\pi$ 行动所能获得的期望累计奖励的估计。价值函数的作用主要包括：

    - **长期规划**：价值函数考虑了未来奖励的期望，帮助智能体做出不仅有利于当前，也有利于未来的决策。
    - **指导策略优化**：价值函数可以用来评估和比较不同策略的好坏，从而指导策略的优化。
    - **缓解稀疏奖励问题**：即使奖励信号很稀疏，价值函数可以通过对环境的探索和学习，逐步构建出对长期奖励的估计。

3. ### 价值函数与奖励信号的结合
  
    在实际的强化学习算法中，价值函数和奖励信号是结合使用的。奖励信号是价值函数更新的基础，而价值函数则为智能体提供了长期的决策依据。例如，在时序差分（TD）学习中，智能体通过比较当前奖励和价值函数的预测来更新价值函数。

