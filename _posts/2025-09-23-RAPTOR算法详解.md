---
layout: post
toc: true
title: "技术学习: RAPTOR 算法"
categories: LLM
tags: [NLP, LLM, RAG]
author:
  - vortezwohl
  - 吴子豪
---

LLMs 凭借大规模参数可编码海量知识, 在各类任务中表现优异, 但存在两大核心局限: 1. 知识固化与更新困难, 参数化知识无法实时跟进世界变化, 且针对特定领域的知识不足, 通过后训练更新知识的成本极高; 2. 知识溯源和可解释性差, LLM 生成内容的知识来源不明, 无法追溯具体文本片段, 不利于需要可信度验证的场景. 为解决上述问题, 检索增强语言模型 (RALMs) 成为一个重要的研究方向, 而 **RAPTOR** (Recursive Abstractive Processing for Tree-Organized Retrieval)是一种基于递归树结构的检索增强框架，旨在解决传统检索增强语言模型无法高效整合长文档多尺度信息的核心问题. 

Paper: [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://doi.org/10.48550/arXiv.2401.18059)

## RALMs 的研究背景

RALMs 的发展围绕着**检索器**(Retriever), **阅读器**(Reader), **端到端训练**三大组件展开, 关键进展如下:

| 组件         | 核心技术演进                                                                 | 代表工作                                                                 |
|--------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------|
| 检索器       | 从传统 term 匹配（TF-IDF、BM25）转向深度学习密集检索（Dense Retrieval）       | DPR (Dense Passage Retrieval)$^{[1]}$、ColBERT (Context Late Interaction)$^{[2]}$、SBERT (句子级嵌入)$^{[3]}$ |
| 阅读器       | 从单块处理转向多块融合，提升长文本理解能力                                   | FiD (Fusion-in-Decoder，独立编码多块后融合)$^{[5]}$、RETRO (跨块注意力+块级检索)$^{[5]}$          |
| 端到端训练   | 联合优化检索器与阅读器，减少组件间误差传递                                   | RAG（Retrieval-Augmented Generation）、Atlas(编码器-解码器 + 检索器联合微调)$^{[6]}$、REALM (掩码LM微调检索)$^{[7]}$ |
| 分层检索     | 尝试通过文档-段落两级检索提升效率，但未解决语义关联问题                       | DHR (Dense Hierarchical Retrieval)$^{[8]}$、HHR (Hybrid Hierarchical Retrieval)$^{[9]}$        |
| 递归摘要     | 用递归摘要捕捉长文本主题，但依赖相邻块分组，忽略远距离语义关联                 | Wu et al. (2021) (递归摘要书籍)$^{[10]}$、LlamaIndex (保留中间节点但按相邻块分组)$^{[11]}$         |

## RALMs 已有研究的局限性

尽管 RALMs 已取得显著进展，传统方案仍存在长文档多尺度信息整合能力不足的关键痛点，具体表现为：

1. **检索单元局限**: 短连续块无法捕捉语篇结构

    传统检索仅获取短且连续的文本块, 无法整合长文档中分散的语义关联, 例如面对主题型问题如 "灰姑娘如何获得幸福结局?", 需整合 "仙女教母帮助 -> 参加舞会 -> 丢失水晶鞋 -> 王子寻找 -> 最终团聚" 等多片段信息, 而单块信息只能覆盖其中某一环节, 导致 LLM 无法形成完整推理链.

2. **语义分割缺陷**: 连续分组忽略远距关联

    现有分层检索或递归摘要方案多基于文本位置相邻性分组 (如按章节、段落顺序)$^{[10]}$，而非语义相似度, 可能将语义相关但位置遥远的块（如同一主题的不同章节内容）拆分到不同组，导致摘要丢失全局主题. 连续分割可能切断句子级语义连贯性（如将一个完整事件拆分为两个块），导致检索片段上下文缺失，甚至产生误导（如技术文档中仅检索到结论而无前提假设）.

3. **长上下文模型的性能瓶颈**: 部分 LLM（如 LongT5、Longformer）支持超长上下文（如 16k+ token），但存在两大问题：

    1. **性能衰减**: 随着上下文长度增加，模型对远距离信息的利用率显著下降$^{[8]}$，尤其当关键信息嵌入长文本中时，推理准确率骤降.

    2. **成本过高**: 长上下文处理的计算复杂度（时间、显存）呈线性甚至超线性增长，无法大规模应用于海量长文档（如书籍、论文库）.

## RAPTOR 算法

...

## 参考文献

[[1](https://doi.org/10.48550/arXiv.2004.04906)] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. *arXiv preprint*, 2020.

[[2](https://doi.org/10.48550/arXiv.2004.12832)] Omar Khattab, Matei Zaharia. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. *arXiv preprint*, 2020.

[[3](https://doi.org/10.48550/arXiv.1908.10084)] Nils Reimers, Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *arXiv preprint*, 2019.

[[4](https://doi.org/10.18653/v1/2021.eacl-main.74)] Gautier Izacard, Edouard Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics*, 2021.

[[5](https://doi.org/10.48550/arXiv.2112.04426)] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre. Improving language models by retrieving from trillions of tokens. *arXiv preprint*, 2022.

[[6](https://doi.org/10.48550/arXiv.2208.03299)] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave. Atlas: Few-shot Learning with Retrieval Augmented Language Models. *arXiv preprint*, 2022.

[[7](https://doi.org/10.48550/arXiv.2002.08909)] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. REALM: Retrieval-Augmented Language Model Pre-Training. *arXiv preprint*, 2020.

[[8](https://doi.org/10.48550/arXiv.2110.15439)] Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, Philip S. Yu. Dense Hierarchical Retrieval for Open-Domain Question Answering. *arXiv preprint*, 2021.

[[9](https://doi.org/10.18653/v1/2023.findings-acl.679)] Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, Zhiheng Huang. Hybrid Hierarchical Retrieval for Open-Domain Question Answering. *Findings of the Association for Computational Linguistics*, 2023.

[[10](https://doi.org/10.48550/arXiv.2109.10862)] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, Paul Christiano. Recursively Summarizing Books with Human Feedback. *arXiv preprint*, 2021.

[[11]()] Liu et al. LlamaIndex. *Github*, 2022.