---
layout: post
toc: false
title: "VAE, 变分自编码器"
categories: ImageGeneration
tags: [ImageGeneration, deeplearning]
author:
  - vortezwohl
  - 吴子豪
---
变分自编码器（Variational Autoencoder, VAE）是一种基于神经网络的生成模型，结合了自编码器（Autoencoder）和变分推断（Variational Inference）的思想，旨在学习样本数据的潜在分布。

## 背景知识

### Variational Inference (变分推断)

变分推断（Variational Inference, VI）是一种用于近似复杂概率分布的技术，其核心思想是将统计推断问题转化为优化问题。

- #### 定义

    变分推断的目标是用一个简单的分布 $q(\theta)$ 近似一个难以直接计算的后验分布 $p(\theta \mid x)$. 通过最小化两者的[ KL 散度](https://vortezwohl.github.io/nlp/2025/04/11/KL%E6%95%A3%E5%BA%A6%E7%9A%84%E9%9D%9E%E5%AF%B9%E7%A7%B0%E6%80%A7%E8%B4%A8.html)来找到最优的 $q(\theta)$:

    $$
    KL(q(\theta) \mid p(\theta \mid x)) = \int q(\theta) \log \frac{q(\theta)}{p(\theta \mid x)} d\theta
    $$

    这种方法把推断问题转化为优化问题.


### Evidence Lower Bound (ELBO):

变分推断的优化目标通常基于 ELBO, 最大化 ELBO 等价于最小化 KL 散度.

- #### 定义

    $$
    \mathcal{L}(q) = \mathbb{E}_{q(\theta)}[\log p(x, \theta)] - \mathbb{E}_{q(\theta)}[\log q(\theta)]
    $$

### Mean Field Approximation (均值场假设)

为了简化计算, 变分推断通常假设近似分布 $q(\theta)$ 是独立因子的乘积:

$$
q(\theta) = \prod_i q_i(\theta_i)
$$

这种假设虽然可能忽略变量之间的依赖关系，但显著降低了计算复杂度.

## 架构

- Encoder 编码器: 将输入数据映射到潜在空间中的均值(mean)和方差(variance), 类似于压缩器, 用于提取数据中的核心表征.

- Decoder 解码器: 从潜在空间中的点映射回原始样本空间, 类似于解压缩, 尽可能恢复数据的细节

## 理论假设

VAE 假设潜在变量遵循某种概率分布（通常是高斯分布），并通过变分推断来学习这种分布。具体来说，VAE 的目标是学习一个潜在空间的分布，使得从该分布中采样的数据能够生成与原始数据相似的新样本。

## 原理

1. ### 编码

    对每个输入样本, 编码器输出两个向量: 均值 $\mu$ 和方差 $\sigma^2$ 的对数.

2. ### 采样

    通过重参数化技巧, 从编码器输出的分布中采样潜在变量 $z$, 重参数化技巧允许在训练过程中通过梯度下降优化采样过程.

3. ### 解码器重建

    解码器根据采样的潜在变量 $z$ 重建输入数据

## 训练数据

VAE 通常采用图像数据集进行训练, 数据中的每个样本都通过编码解码器进行处理.
