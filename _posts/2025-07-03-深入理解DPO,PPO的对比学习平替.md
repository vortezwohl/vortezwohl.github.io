---
layout: post
toc: true
title: "深入理解 DPO: PPO 的对比学习平替"
categories: NLP
tags: [NLP, DeepLearning]
author:
  - vortezwohl
  - 吴子豪
---
DPO (Direct Preference Optimization，直接偏好优化) 是一种对比学习方法，其通过对每条提示提供两条不同的答案，并给出这两个答案的偏好偏序，让模型输出更接近优质答案，同时更远离劣质答案。其目标是提高优质答案相对于错误答案的相对对数概率。它利用成对的偏好数据集，通过一个巧妙的损失函数，直接调整大语言模型的概率分布，使其提高生成优质答案的概率，降低生成劣质答案的概率。这本质上是在进行一种隐式的奖励建模和优化。

# 深入理解 DPO

DPO 最初由 Anthropic 的研究团队于 2023 年 5 月 29 日公开于 [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://doi.org/10.48550/arXiv.2305.18290)，DPO 的提出源于对传统 RLHF（Reinforcement Learning from Human Feedback）方法的反思。

## 研究背景

RLHF (主要是 PPO 算法) 是当时的主流对齐人类偏好的技术，其流程包括：

1. **监督微调 (SFT)**: 用问答对对语言模型进行监督微调，让语言模型学习输出模式。

2. **奖励模型训练 (Reward Modeling)**: 使用正负样本对训练文本分类模型 (通常是 BERT + Sigmoid head)

3. **强化学习 (RL)**: 语言模型作为策略模型，使用奖励模型对语言模型的输出生成奖励信号，指导模型的权重更新。

以上方法虽然流行了很长一段时间，但其存在以下问题：

- **训练过程繁琐**：需要多阶段训练，流程周期长。

- **稳定性差**：RL 过程容易出现训练不稳定，奖励作弊等问题，导致效果差强人意。

- **样本效率低**：需要大量问答对数据用于训练。

- **可解释性弱**：RL 的优化目标和人类偏好的联系并不直接。

## 创新点

DPO 旨在直接优化语言模型以匹配人类偏好，而无需显式训练奖励模型，从而解决 RLHF 的痛点：

- **简化流程**：不再需要训练文本分类奖励模型，也不再需要强化学习，直接基于偏好数据优化模型。

- **提高稳定性**：避免了 RL 带来的训练过程不稳定和奖励作弊问题。

- **提升样本效率**：更少的训练时间和数据需求。

- **更强的可解释性**：目标函数直接对应人类偏好。

### 目标函数

DPO 的核心创新在于设计了直接最大化偏好一致性的目标函数：

$$
L(θ) = - E[(x,r,j)~D] [log_{\pi_θ}(r|x) - log_{\pi_θ}(j|x) - \beta * (log_{\pi_\phi}(r|x) - log_{\pi_\phi}(j|x))]
$$

其中，$x$ 是提示词，$r$ 是偏好回答，$j$ 是不被偏好的回答，$\pi_\theta$ 是待优化的目标模型，$\pi_\phi$ 是参考模型，$\beta$ 是温度参数，控制目标模型与参考模型的对齐程度。

DPO 的目标是直接优化偏好回答和不被偏好的回答的相对概率，在训练的过程中，模型需要提高偏好回答的概率，同时保持与参考模型的一致性 (通过 KL 散度约束)，以避免过度优化导致退化。

DPO 目标函数的核心组成是 $log_{\pi_θ}(r|x) - log_{\pi_θ}(j|x)$，通过最大化该表达式，实现了对偏好回答概率的最大化，和对非偏好回答概率的最小化。

原 Paper 在理论上证明了: DPO 目标函数等价于最小化模型策略与人类偏好的 KL 散度，为算法提供了理论后盾。

## 局限性

- 依赖参考模型：性能仍受参考模型质量的影响。

- $\beta$ 参数敏感性：温度参数 $\beta$ 需要仔细调优，否则可能导致性能下降。

- 不适用于所有任务：在某些复杂任务，例如长文本生成，PPO 则仍更具优势。

- 局部最优：DPO 摒弃了 RL，没有了 RL 的探索，策略可能陷入局部最优，而非全局最优。
